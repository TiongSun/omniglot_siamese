{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese neural networks for omniglot dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Omniglot dataset\n",
    "- 1623 hand drawn characters\n",
    "- 50 alphabets\n",
    "- 20 examples for each character\n",
    "- image resolution 105 x 105\n",
    "\n",
    "Training Data\n",
    "- 964 characters\n",
    "- 19,280 images\n",
    "\n",
    "Validation Data\n",
    "- 659 characters\n",
    "- 13,180 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "- Pickle the omniglot data for cleaner structure and easier data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# general packages\n",
    "import numpy as np\n",
    "from imageio import imread\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Keras packages\n",
    "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = 'C:/Users/hydro/Desktop/AI/omniglot' \n",
    "data_path = '/home/ubuntu/omniglot/python'\n",
    "train_path = os.path.join(data_path,'images_background') # 30 alphabets\n",
    "val_path = os.path.join(data_path,'images_evaluation')  # 20 alphabets\n",
    "weights_path = os.path.join(data_path, \"weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define load function to \n",
    "- load data in an array: (characters, images, width , height)\n",
    "- store index into a dictionary: {language: [start_index, end_index]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_omniglot(path):\n",
    "\n",
    "    image_array = [] # store image data etc in an array. Eg. (characters, images, width , height)\n",
    "    language_dict = {} # store language and its index. Eg. {'Alphabet_of_the_Magi': [0, 19]}\n",
    "    count = 0\n",
    "    \n",
    "    # For each language (eg. 'Balinese' ...)\n",
    "    for language in os.listdir(path): \n",
    "        \n",
    "        language_dict[language] = [count, None]\n",
    "        language_path = os.path.join(path, language)\n",
    "        \n",
    "        # For each character within the language folder (eg. letter = 'character01' , 'character02'...)\n",
    "        for character in os.listdir(language_path): \n",
    "            character_path = os.path.join(language_path, character)\n",
    "            \n",
    "            # empty list to store all the images of the particular character\n",
    "            image_list = []\n",
    "            \n",
    "            # For each individual image of the character (eg. 0299_01, 0299_02)\n",
    "            for image in os.listdir(character_path):\n",
    "                image_path = os.path.join(character_path, image)\n",
    "                \n",
    "                # image to numpy array\n",
    "                image = imread(image_path) \n",
    "                \n",
    "                # store image in the list\n",
    "                image_list.append(image)\n",
    "                \n",
    "            image_array.append(np.stack(image_list))\n",
    "\n",
    "            language_dict[language][1] = count\n",
    "            \n",
    "            count += 1\n",
    "\n",
    "    image_array = np.stack(image_array) \n",
    "    \n",
    "    return image_array, language_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle the arrays and dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array, train_language_dict = load_omniglot(train_path)\n",
    "with open(os.path.join(data_path,\"train.pickle\"), \"wb\") as f:\n",
    "\tpickle.dump((train_array,train_language_dict),f)\n",
    "\n",
    "\n",
    "val_array, val_language_dict = load_omniglot(val_path)\n",
    "with open(os.path.join(data_path,\"val.pickle\"), \"wb\") as f:\n",
    "\tpickle.dump((val_array,val_language_dict),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the array shapes and dictionaries to double check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training array shape =  (964, 20, 105, 105)\n",
      "\n",
      "Training language dictionary =  {'Alphabet_of_the_Magi': [0, 19], 'Anglo-Saxon_Futhorc': [20, 48], 'Arcadian': [49, 74], 'Armenian': [75, 115], 'Asomtavruli_(Georgian)': [116, 155], 'Balinese': [156, 179], 'Bengali': [180, 225], 'Blackfoot_(Canadian_Aboriginal_Syllabics)': [226, 239], 'Braille': [240, 265], 'Burmese_(Myanmar)': [266, 299], 'Cyrillic': [300, 332], 'Early_Aramaic': [333, 354], 'Futurama': [355, 380], 'Grantha': [381, 423], 'Greek': [424, 447], 'Gujarati': [448, 495], 'Hebrew': [496, 517], 'Inuktitut_(Canadian_Aboriginal_Syllabics)': [518, 533], 'Japanese_(hiragana)': [534, 585], 'Japanese_(katakana)': [586, 632], 'Korean': [633, 672], 'Latin': [673, 698], 'Malay_(Jawi_-_Arabic)': [699, 738], 'Mkhedruli_(Georgian)': [739, 779], 'N_Ko': [780, 812], 'Ojibwe_(Canadian_Aboriginal_Syllabics)': [813, 826], 'Sanskrit': [827, 868], 'Syriac_(Estrangelo)': [869, 891], 'Tagalog': [892, 908], 'Tifinagh': [909, 963]}\n",
      "\n",
      "Validation array shape =  (659, 20, 105, 105)\n",
      "\n",
      "Validation language dictionary =  {'Angelic': [0, 19], 'Atemayar_Qelisayer': [20, 45], 'Atlantean': [46, 71], 'Aurek-Besh': [72, 97], 'Avesta': [98, 123], 'Ge_ez': [124, 149], 'Glagolitic': [150, 194], 'Gurmukhi': [195, 239], 'Kannada': [240, 280], 'Keble': [281, 306], 'Malayalam': [307, 353], 'Manipuri': [354, 393], 'Mongolian': [394, 423], 'Old_Church_Slavonic_(Cyrillic)': [424, 468], 'Oriya': [469, 514], 'Sylheti': [515, 542], 'Syriac_(Serto)': [543, 565], 'Tengwar': [566, 590], 'Tibetan': [591, 632], 'ULOG': [633, 658]}\n"
     ]
    }
   ],
   "source": [
    "print('\\nTraining array shape = ', train_array.shape)\n",
    "print('\\nTraining language dictionary = ', train_language_dict)\n",
    "    \n",
    "print('\\nValidation array shape = ', val_array.shape)\n",
    "print('\\nValidation language dictionary = ', val_language_dict)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The algorithm\n",
    "- The original Lake et al. paper used Hierachical Bayesian Program Learning (GBPL)\n",
    "- HBPL data used stokes, which requires more complicated annotation and is hard to be generalizable\n",
    "- Siamese neural networks are chosen for this challenge because:\n",
    "<br> (1) it is well suited for one-shot-learning problem; and  \n",
    "(2) it can be generalized to other challenges beyond Omniglot\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese neural networks\n",
    "- Siamese neural networks for one-shot image recognition was proposed in [Koch et al. paper](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)\n",
    "- The networks are good for diffentiating two images. \n",
    "- Its architecture consists of two identical neural nets that share the same weights. \n",
    "- Each image in an image pair is feeded into one of the two neural nets and the differences between the two vectors are used as inputs for a logistic regression classifier\n",
    "- The loss function used for logistic regression is binary cross entropy.\n",
    "- L2 regularization is used to prevent over-fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"siamese.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and biases are initialized according to [Koch et al. paper](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)\n",
    "\n",
    "#### Weight initialization\n",
    "- convolutional layers & fully connected layer: \n",
    "<br> (1) normal distribution\n",
    "<br> (2) mean = 0\n",
    "<br> (3) standard deviation = 10^-2\n",
    "\n",
    "#### Bias initialization\n",
    "- convolutional layers & fully connected layer:\n",
    "<br> (1) normal distribution\n",
    "<br> (2) mean = 0.5\n",
    "<br> (3) standard deviation = 10^2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_initializer(shape, name=None):\n",
    "    values = np.random.normal(loc=0, scale=1e-2, size=shape)\n",
    "    return K.variable(values, name=name)\n",
    "\n",
    "def bias_initializer(shape, name=None):\n",
    "    values = np.random.normal(loc=0.5, scale=1e-2, size=shape)\n",
    "    return K.variable(values ,name=name)\n",
    "\n",
    "# Omniglot data input shape\n",
    "input_shape = (105, 105, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks\n",
    "\n",
    "Define the structure for CNN used\n",
    "<img src='cnn.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Networks\n",
    "convnet = Sequential()\n",
    "\n",
    "# First convolutional layer\n",
    "convnet.add(Conv2D(64,(10,10),\n",
    "           activation='relu',\n",
    "           input_shape=input_shape,\n",
    "           kernel_initializer=weight_initializer,\n",
    "           kernel_regularizer=l2(2e-4)))\n",
    "convnet.add(MaxPooling2D())\n",
    "\n",
    "# Second convolutional layer\n",
    "convnet.add(Conv2D(128,(7,7),\n",
    "           activation='relu',\n",
    "           kernel_regularizer=l2(2e-4),\n",
    "           kernel_initializer=weight_initializer,\n",
    "           bias_initializer=bias_initializer))\n",
    "convnet.add(MaxPooling2D())\n",
    "\n",
    "# Third convolutional layer\n",
    "convnet.add(Conv2D(128,(4,4),\n",
    "           activation='relu',\n",
    "           kernel_initializer=weight_initializer,\n",
    "           kernel_regularizer=l2(2e-4),\n",
    "           bias_initializer=bias_initializer))\n",
    "convnet.add(MaxPooling2D())\n",
    "\n",
    "# Fourth convolutional layer\n",
    "convnet.add(Conv2D(256,(4,4),\n",
    "           activation='relu',\n",
    "           kernel_initializer=weight_initializer,\n",
    "           kernel_regularizer=l2(2e-4),\n",
    "           bias_initializer=bias_initializer))\n",
    "convnet.add(Flatten())\n",
    "\n",
    "# Fully connected layer\n",
    "convnet.add(Dense(4096,\n",
    "          activation=\"sigmoid\",\n",
    "          kernel_regularizer=l2(1e-3),\n",
    "          kernel_initializer=weight_initializer,\n",
    "          bias_initializer=bias_initializer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assign two images in a pair as left or right inputs and convert to tensors\n",
    "- L1 component-wise distance between two vectors are calculated after the fully connected layer\n",
    "- Optimization: Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert input images into tensors\n",
    "left_input = Input(input_shape)\n",
    "right_input = Input(input_shape)\n",
    "\n",
    "# call the convnet Sequential model on each of the input tensors so params will be shared\n",
    "encoded_l = convnet(left_input)\n",
    "encoded_r = convnet(right_input)\n",
    "\n",
    "# layer to merge two encoded inputs with the l1 distance between them\n",
    "L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "# call this layer on list of two input tensors.\n",
    "prediction = Dense(1,activation='sigmoid',bias_initializer=bias_initializer)(L1_distance)\n",
    "\n",
    "siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "\n",
    "siamese_net.compile(loss = \"binary_crossentropy\", optimizer = Adam(0.00006))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training loop\n",
    "evaluate_every = 10 # interval for evaluating on one-shot tasks\n",
    "loss_every = 10 # interval for printing loss (iterations)\n",
    "batch_size = 32 # how many pair of images for each batch\n",
    "n_iter =  10000 \n",
    "N_character = 20 # how many languages for testing one-shot tasks\n",
    "n_val = 250 # how many one-shot tasks to validate on\n",
    "best = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Siamese neural network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siamese_Loader:\n",
    "    \"\"\"For loading batches and testing tasks to a siamese net\"\"\"\n",
    "    def __init__(self, path, train_val = [\"train\", \"val\"]):\n",
    "        self.data = {}\n",
    "        self.language = {}\n",
    "        self.info = {}\n",
    "        \n",
    "        # load training and validation array              --->   self.data dictionary\n",
    "        # load training and validation index dictionaries --->   self.categories dictionary \n",
    "        \n",
    "        for t_v in train_val:\n",
    "            file_path = os.path.join(path, t_v + \".pickle\")\n",
    "\n",
    "            with open(file_path,\"rb\") as f:\n",
    "                (data_array,language_dict) = pickle.load(f)\n",
    "                self.data[t_v] = data_array\n",
    "                self.language[t_v] = language_dict\n",
    "\n",
    "    def get_batch(self,batch_size, train_val = \"train\"):\n",
    "        \"\"\"Create batch of n pairs, half same class, half different class\"\"\"\n",
    "        \n",
    "        # load training or validation data into data array dictionary \n",
    "        data_array = self.data[train_val]\n",
    "        \n",
    "        # Extract the training dataset parameters\n",
    "        n_character, n_example, weight, height = data_array.shape\n",
    "\n",
    "        # randomly sample several characters to use in the batch\n",
    "        random_character_batch = np.random.choice(n_character,size=(batch_size,),replace=False)\n",
    "        \n",
    "        # initialize 2 empty arrays for the input image batch\n",
    "        pairs = [np.zeros((batch_size, height, weight, 1)) for i in range(2)]\n",
    "        \n",
    "        # initialize vector for the targets\n",
    "        # make one half of it '1's, so 2nd half of batch has same class\n",
    "        targets = np.zeros((batch_size,))\n",
    "        targets[batch_size//2:] = 1\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # choose one character\n",
    "            one_character = random_character_batch[i]\n",
    "            \n",
    "            # First image in the pair \n",
    "            # Generate a random list of integers\n",
    "            idx_1 = np.random.randint(0, n_example)\n",
    "             \n",
    "            # First image in the pair\n",
    "            # Array data of a randomly selected image is loaded onto pairs[0]\n",
    "            pairs[0][i,:,:,:] = data_array[one_character, idx_1].reshape(weight, height, 1)\n",
    "            \n",
    "            # pick images of same language for 1st half, different for 2nd\n",
    "            if i >= batch_size // 2:\n",
    "                character_2 = one_character \n",
    "                \n",
    "            else: \n",
    "                #add a random number to the category modulo n classes to ensure 2nd image has\n",
    "                # ..different category\n",
    "                character_2 = (one_character + np.random.randint(1, n_character)) % n_character\n",
    "                \n",
    "                \n",
    "            idx_2 = np.random.randint(0, n_example)\n",
    "            \n",
    "            pairs[1][i,:,:,:] = data_array[character_2,idx_2].reshape(weight, height, 1)\n",
    "            \n",
    "        return pairs, targets\n",
    "    \n",
    "    def generate(self, batch_size, train_val = \"train\"):\n",
    "        \"\"\"a generator for batches, so model.fit_generator can be used. \"\"\"\n",
    "        \n",
    "        while True:\n",
    "            pairs, targets = self.get_batch(batch_size, train_val)\n",
    "            yield (pairs, targets)    \n",
    "\n",
    "    def make_oneshot_task(self, N_character, train_val = \"val\", language = None):\n",
    "        \"\"\"Create pairs of test image, support set for testing N character one-shot learning. \"\"\"\n",
    "        \n",
    "        # load validation data into data array dictionary \n",
    "        data_array = self.data[train_val]\n",
    "        \n",
    "        # Extract the validation dataset parameters\n",
    "        n_character, n_example, weight, height = data_array.shape\n",
    "        \n",
    "        # generate random indices\n",
    "        indices = np.random.randint(0, n_example, size=(N_character,))\n",
    "        \n",
    "        # if a language is specified, use only character from that language\n",
    "        if language is not None:\n",
    "            # get the highest and lowest indexes of that particulra language\n",
    "            low_index, high_index = self.language[train_val][language]\n",
    "            \n",
    "            # if the total characters in the language is less than specified, print error\n",
    "            if N_character > high_index - low_index:\n",
    "                raise ValueError(\"This language ({}) has less than {} characters\".format(language, N_character))\n",
    "                \n",
    "            random_character = np.random.choice(range(low_index, high_index),size=(N_character,),replace=False)\n",
    "        \n",
    "        # if no language specified just pick a bunch of random letters\n",
    "        else:\n",
    "            \n",
    "            random_character = np.random.choice(range(n_character),size=(N_character,),replace=False) \n",
    "            \n",
    "        true_category = random_character[0]\n",
    "        ex1, ex2 = np.random.choice(n_example, replace=False, size=(2,))\n",
    "        test_image = np.asarray([data_array[true_category,ex1,:,:]] * N_character).reshape(N_character, weight, height, 1)\n",
    "        \n",
    "        # support set is a set of one image from each characters\n",
    "        # to be compared with the target image\n",
    "        support_set = data_array[random_character, indices,:,:]\n",
    "        support_set[0,:,:] = data_array[true_category, ex2]\n",
    "        support_set = support_set.reshape(N_character, weight, height, 1)\n",
    "        \n",
    "        # targets = the test image\n",
    "        targets = np.zeros((N_character,))\n",
    "        targets[0] = 1\n",
    "        targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
    "        \n",
    "        pairs = [test_image,support_set]\n",
    "\n",
    "        return pairs, targets\n",
    "    \n",
    "    def test_oneshot(self,model, N_character, n_val, train_val =\"val\"):\n",
    "        \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over n one-shot tasks\"\"\"\n",
    "        \n",
    "        n_correct = 0\n",
    "        \n",
    "        for i in range(n_val):\n",
    "            inputs, targets = self.make_oneshot_task(N_character,train_val)\n",
    "            probs = model.predict(inputs)\n",
    "            \n",
    "            if np.argmax(probs) == np.argmax(targets):\n",
    "                n_correct += 1\n",
    "                \n",
    "        percent_correct = (100.0*n_correct / n_val)\n",
    "        \n",
    "        return percent_correct\n",
    "    \n",
    "    def train(self, model, epochs, verbosity):\n",
    "        model.fit_generator(self.generate(batch_size),)\n",
    "    \n",
    "    \n",
    "#Instantiate the class\n",
    "loader = Siamese_Loader(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "- Using a batch size of 32\n",
    "- evalute loss and accuracy for every 10 iteration\n",
    "- Accuracy = Randomly selected 250 images from any language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "iteration 10, training loss: 3.06, accuracy: 19.2\n",
      "iteration 20, training loss: 3.04, accuracy: 26.8\n",
      "iteration 30, training loss: 3.01, accuracy: 29.6\n",
      "iteration 40, training loss: 2.89, accuracy: 16.8\n",
      "iteration 50, training loss: 2.84, accuracy: 28.4\n",
      "iteration 60, training loss: 2.86, accuracy: 27.2\n",
      "iteration 70, training loss: 2.61, accuracy: 31.2\n",
      "iteration 80, training loss: 2.71, accuracy: 25.2\n",
      "iteration 90, training loss: 2.52, accuracy: 20.4\n",
      "iteration 100, training loss: 2.60, accuracy: 28.8\n",
      "iteration 110, training loss: 2.52, accuracy: 34.4\n",
      "iteration 120, training loss: 2.41, accuracy: 26.4\n",
      "iteration 130, training loss: 2.40, accuracy: 30.4\n",
      "iteration 140, training loss: 2.43, accuracy: 32.0\n",
      "iteration 150, training loss: 2.45, accuracy: 32.8\n",
      "iteration 160, training loss: 2.12, accuracy: 26.0\n",
      "iteration 170, training loss: 2.06, accuracy: 33.6\n",
      "iteration 180, training loss: 2.18, accuracy: 30.8\n",
      "iteration 190, training loss: 2.18, accuracy: 28.0\n",
      "iteration 200, training loss: 2.06, accuracy: 36.8\n",
      "iteration 210, training loss: 2.12, accuracy: 34.0\n",
      "iteration 220, training loss: 1.99, accuracy: 33.2\n",
      "iteration 230, training loss: 2.01, accuracy: 39.6\n",
      "iteration 240, training loss: 2.07, accuracy: 38.0\n",
      "iteration 250, training loss: 1.93, accuracy: 31.2\n",
      "iteration 260, training loss: 2.00, accuracy: 45.2\n",
      "iteration 270, training loss: 1.84, accuracy: 36.4\n",
      "iteration 280, training loss: 1.87, accuracy: 40.0\n",
      "iteration 290, training loss: 1.91, accuracy: 43.6\n",
      "iteration 300, training loss: 1.89, accuracy: 41.6\n",
      "iteration 310, training loss: 1.74, accuracy: 38.0\n",
      "iteration 320, training loss: 1.75, accuracy: 40.0\n",
      "iteration 330, training loss: 1.72, accuracy: 42.4\n",
      "iteration 340, training loss: 1.66, accuracy: 39.6\n",
      "iteration 350, training loss: 1.61, accuracy: 44.4\n",
      "iteration 360, training loss: 1.76, accuracy: 45.2\n",
      "iteration 370, training loss: 1.75, accuracy: 46.8\n",
      "iteration 380, training loss: 1.64, accuracy: 46.8\n",
      "iteration 390, training loss: 1.73, accuracy: 40.4\n",
      "iteration 400, training loss: 1.62, accuracy: 42.0\n",
      "iteration 410, training loss: 1.54, accuracy: 42.0\n",
      "iteration 420, training loss: 1.60, accuracy: 48.4\n",
      "iteration 430, training loss: 1.43, accuracy: 39.2\n",
      "iteration 440, training loss: 1.52, accuracy: 41.6\n",
      "iteration 450, training loss: 1.71, accuracy: 40.4\n",
      "iteration 460, training loss: 1.79, accuracy: 36.8\n",
      "iteration 470, training loss: 1.60, accuracy: 45.2\n",
      "iteration 480, training loss: 1.55, accuracy: 44.0\n",
      "iteration 490, training loss: 1.60, accuracy: 38.0\n",
      "iteration 500, training loss: 1.47, accuracy: 51.2\n",
      "iteration 510, training loss: 1.50, accuracy: 51.2\n",
      "iteration 520, training loss: 1.34, accuracy: 52.0\n",
      "iteration 530, training loss: 1.54, accuracy: 45.6\n",
      "iteration 540, training loss: 1.30, accuracy: 50.0\n",
      "iteration 550, training loss: 1.57, accuracy: 45.2\n",
      "iteration 560, training loss: 1.42, accuracy: 43.6\n",
      "iteration 570, training loss: 1.39, accuracy: 48.0\n",
      "iteration 580, training loss: 1.31, accuracy: 52.8\n",
      "iteration 590, training loss: 1.28, accuracy: 48.0\n",
      "iteration 600, training loss: 1.29, accuracy: 56.0\n",
      "iteration 610, training loss: 1.32, accuracy: 56.0\n",
      "iteration 620, training loss: 1.32, accuracy: 52.8\n",
      "iteration 630, training loss: 1.41, accuracy: 47.2\n",
      "iteration 640, training loss: 1.21, accuracy: 53.6\n",
      "iteration 650, training loss: 1.21, accuracy: 50.0\n",
      "iteration 660, training loss: 1.19, accuracy: 51.2\n",
      "iteration 670, training loss: 1.26, accuracy: 54.8\n",
      "iteration 680, training loss: 1.28, accuracy: 45.6\n",
      "iteration 690, training loss: 1.38, accuracy: 56.4\n",
      "iteration 700, training loss: 1.26, accuracy: 57.2\n",
      "iteration 710, training loss: 1.19, accuracy: 51.2\n",
      "iteration 720, training loss: 1.06, accuracy: 53.6\n",
      "iteration 730, training loss: 1.14, accuracy: 51.6\n",
      "iteration 740, training loss: 1.11, accuracy: 56.8\n",
      "iteration 750, training loss: 1.16, accuracy: 56.4\n",
      "iteration 760, training loss: 1.22, accuracy: 52.8\n",
      "iteration 770, training loss: 1.14, accuracy: 56.4\n",
      "iteration 780, training loss: 1.07, accuracy: 59.6\n",
      "iteration 790, training loss: 1.26, accuracy: 51.6\n",
      "iteration 800, training loss: 1.13, accuracy: 54.4\n",
      "iteration 810, training loss: 1.17, accuracy: 55.6\n",
      "iteration 820, training loss: 1.17, accuracy: 47.6\n",
      "iteration 830, training loss: 1.09, accuracy: 63.6\n",
      "iteration 840, training loss: 1.03, accuracy: 46.4\n",
      "iteration 850, training loss: 1.00, accuracy: 58.0\n",
      "iteration 860, training loss: 0.93, accuracy: 54.4\n",
      "iteration 870, training loss: 1.04, accuracy: 56.4\n",
      "iteration 880, training loss: 1.24, accuracy: 54.4\n",
      "iteration 890, training loss: 1.09, accuracy: 56.0\n",
      "iteration 900, training loss: 1.01, accuracy: 47.6\n",
      "iteration 910, training loss: 1.08, accuracy: 55.6\n",
      "iteration 920, training loss: 1.11, accuracy: 59.2\n",
      "iteration 930, training loss: 0.99, accuracy: 56.0\n",
      "iteration 940, training loss: 1.03, accuracy: 61.2\n",
      "iteration 950, training loss: 0.94, accuracy: 61.6\n",
      "iteration 960, training loss: 0.94, accuracy: 62.0\n",
      "iteration 970, training loss: 0.95, accuracy: 58.4\n",
      "iteration 980, training loss: 1.00, accuracy: 60.4\n",
      "iteration 990, training loss: 0.99, accuracy: 55.6\n",
      "iteration 1000, training loss: 1.03, accuracy: 59.2\n",
      "iteration 1010, training loss: 0.98, accuracy: 60.8\n",
      "iteration 1020, training loss: 1.00, accuracy: 61.2\n",
      "iteration 1030, training loss: 0.88, accuracy: 58.0\n",
      "iteration 1040, training loss: 0.99, accuracy: 61.2\n",
      "iteration 1050, training loss: 0.88, accuracy: 54.4\n",
      "iteration 1060, training loss: 1.08, accuracy: 60.0\n",
      "iteration 1070, training loss: 0.89, accuracy: 58.8\n",
      "iteration 1080, training loss: 0.90, accuracy: 61.6\n",
      "iteration 1090, training loss: 0.99, accuracy: 58.4\n",
      "iteration 1100, training loss: 0.82, accuracy: 60.8\n",
      "iteration 1110, training loss: 0.89, accuracy: 58.4\n",
      "iteration 1120, training loss: 0.99, accuracy: 62.0\n",
      "iteration 1130, training loss: 0.87, accuracy: 60.4\n",
      "iteration 1140, training loss: 0.89, accuracy: 60.4\n",
      "iteration 1150, training loss: 0.93, accuracy: 54.4\n",
      "iteration 1160, training loss: 0.83, accuracy: 51.2\n",
      "iteration 1170, training loss: 0.83, accuracy: 63.2\n",
      "iteration 1180, training loss: 0.94, accuracy: 58.8\n",
      "iteration 1190, training loss: 0.99, accuracy: 58.8\n",
      "iteration 1200, training loss: 0.82, accuracy: 62.0\n",
      "iteration 1210, training loss: 0.98, accuracy: 58.0\n",
      "iteration 1220, training loss: 0.86, accuracy: 58.4\n",
      "iteration 1230, training loss: 0.83, accuracy: 62.8\n",
      "iteration 1240, training loss: 0.97, accuracy: 54.8\n",
      "iteration 1250, training loss: 0.93, accuracy: 64.8\n",
      "iteration 1260, training loss: 0.75, accuracy: 58.0\n",
      "iteration 1270, training loss: 0.83, accuracy: 65.2\n",
      "iteration 1280, training loss: 0.85, accuracy: 60.4\n",
      "iteration 1290, training loss: 0.88, accuracy: 62.4\n",
      "iteration 1300, training loss: 0.96, accuracy: 63.2\n",
      "iteration 1310, training loss: 0.82, accuracy: 64.4\n",
      "iteration 1320, training loss: 0.82, accuracy: 63.6\n",
      "iteration 1330, training loss: 0.90, accuracy: 62.0\n",
      "iteration 1340, training loss: 0.87, accuracy: 63.2\n",
      "iteration 1350, training loss: 0.91, accuracy: 66.4\n",
      "iteration 1360, training loss: 0.75, accuracy: 61.6\n",
      "iteration 1370, training loss: 0.75, accuracy: 66.0\n",
      "iteration 1380, training loss: 0.75, accuracy: 62.0\n",
      "iteration 1390, training loss: 0.75, accuracy: 66.0\n",
      "iteration 1400, training loss: 0.81, accuracy: 62.0\n",
      "iteration 1410, training loss: 0.88, accuracy: 63.2\n",
      "iteration 1420, training loss: 1.13, accuracy: 67.2\n",
      "iteration 1430, training loss: 0.72, accuracy: 59.6\n",
      "iteration 1440, training loss: 0.85, accuracy: 58.0\n",
      "iteration 1450, training loss: 0.77, accuracy: 70.0\n",
      "iteration 1460, training loss: 0.69, accuracy: 64.0\n",
      "iteration 1470, training loss: 0.91, accuracy: 62.4\n",
      "iteration 1480, training loss: 0.80, accuracy: 59.2\n",
      "iteration 1490, training loss: 0.88, accuracy: 61.6\n",
      "iteration 1500, training loss: 0.66, accuracy: 62.8\n",
      "iteration 1510, training loss: 0.71, accuracy: 58.4\n",
      "iteration 1520, training loss: 0.67, accuracy: 62.4\n",
      "iteration 1530, training loss: 0.79, accuracy: 58.4\n",
      "iteration 1540, training loss: 0.85, accuracy: 61.2\n",
      "iteration 1550, training loss: 0.67, accuracy: 62.8\n",
      "iteration 1560, training loss: 0.76, accuracy: 62.4\n",
      "iteration 1570, training loss: 0.75, accuracy: 63.2\n",
      "iteration 1580, training loss: 0.66, accuracy: 64.0\n",
      "iteration 1590, training loss: 0.65, accuracy: 64.0\n",
      "iteration 1600, training loss: 0.81, accuracy: 68.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1610, training loss: 0.78, accuracy: 69.6\n",
      "iteration 1620, training loss: 0.83, accuracy: 68.0\n",
      "iteration 1630, training loss: 0.73, accuracy: 70.4\n",
      "iteration 1640, training loss: 0.77, accuracy: 64.4\n",
      "iteration 1650, training loss: 0.72, accuracy: 66.4\n",
      "iteration 1660, training loss: 0.62, accuracy: 52.8\n",
      "iteration 1670, training loss: 0.74, accuracy: 62.0\n",
      "iteration 1680, training loss: 0.70, accuracy: 66.0\n",
      "iteration 1690, training loss: 0.77, accuracy: 57.6\n",
      "iteration 1700, training loss: 0.82, accuracy: 62.8\n",
      "iteration 1710, training loss: 1.00, accuracy: 58.0\n",
      "iteration 1720, training loss: 0.63, accuracy: 68.8\n",
      "iteration 1730, training loss: 0.60, accuracy: 63.6\n",
      "iteration 1740, training loss: 0.81, accuracy: 64.8\n",
      "iteration 1750, training loss: 0.59, accuracy: 66.4\n",
      "iteration 1760, training loss: 0.65, accuracy: 68.0\n",
      "iteration 1770, training loss: 0.60, accuracy: 64.0\n",
      "iteration 1780, training loss: 0.83, accuracy: 66.8\n",
      "iteration 1790, training loss: 0.88, accuracy: 66.0\n",
      "iteration 1800, training loss: 0.62, accuracy: 60.4\n",
      "iteration 1810, training loss: 0.75, accuracy: 67.2\n",
      "iteration 1820, training loss: 0.75, accuracy: 61.6\n",
      "iteration 1830, training loss: 0.64, accuracy: 70.4\n",
      "iteration 1840, training loss: 0.64, accuracy: 71.2\n",
      "iteration 1850, training loss: 0.64, accuracy: 60.0\n",
      "iteration 1860, training loss: 0.69, accuracy: 66.4\n",
      "iteration 1870, training loss: 0.73, accuracy: 66.0\n",
      "iteration 1880, training loss: 0.70, accuracy: 66.0\n",
      "iteration 1890, training loss: 0.55, accuracy: 64.0\n",
      "iteration 1900, training loss: 0.56, accuracy: 64.0\n",
      "iteration 1910, training loss: 0.64, accuracy: 62.0\n",
      "iteration 1920, training loss: 0.59, accuracy: 63.6\n",
      "iteration 1930, training loss: 0.64, accuracy: 65.6\n",
      "iteration 1940, training loss: 0.73, accuracy: 68.4\n",
      "iteration 1950, training loss: 0.67, accuracy: 64.4\n",
      "iteration 1960, training loss: 0.60, accuracy: 66.0\n",
      "iteration 1970, training loss: 0.65, accuracy: 70.4\n",
      "iteration 1980, training loss: 0.52, accuracy: 60.8\n",
      "iteration 1990, training loss: 0.63, accuracy: 63.2\n",
      "iteration 2000, training loss: 0.57, accuracy: 66.8\n",
      "iteration 2010, training loss: 0.66, accuracy: 65.2\n",
      "iteration 2020, training loss: 0.70, accuracy: 61.6\n",
      "iteration 2030, training loss: 0.61, accuracy: 60.8\n",
      "iteration 2040, training loss: 0.56, accuracy: 68.4\n",
      "iteration 2050, training loss: 0.59, accuracy: 66.8\n",
      "iteration 2060, training loss: 0.57, accuracy: 61.6\n",
      "iteration 2070, training loss: 0.61, accuracy: 69.6\n",
      "iteration 2080, training loss: 0.58, accuracy: 70.0\n",
      "iteration 2090, training loss: 0.65, accuracy: 68.8\n",
      "iteration 2100, training loss: 0.57, accuracy: 64.0\n",
      "iteration 2110, training loss: 0.59, accuracy: 68.8\n",
      "iteration 2120, training loss: 0.71, accuracy: 63.6\n",
      "iteration 2130, training loss: 0.56, accuracy: 66.8\n",
      "iteration 2140, training loss: 0.71, accuracy: 67.6\n",
      "iteration 2150, training loss: 0.53, accuracy: 69.6\n",
      "iteration 2160, training loss: 0.48, accuracy: 68.8\n",
      "iteration 2170, training loss: 0.71, accuracy: 65.6\n",
      "iteration 2180, training loss: 0.57, accuracy: 64.0\n",
      "iteration 2190, training loss: 0.51, accuracy: 68.0\n",
      "iteration 2200, training loss: 0.52, accuracy: 64.0\n",
      "iteration 2210, training loss: 0.54, accuracy: 64.4\n",
      "iteration 2220, training loss: 0.64, accuracy: 67.2\n",
      "iteration 2230, training loss: 0.49, accuracy: 68.8\n",
      "iteration 2240, training loss: 0.56, accuracy: 70.0\n",
      "iteration 2250, training loss: 0.51, accuracy: 66.8\n",
      "iteration 2260, training loss: 0.55, accuracy: 70.0\n",
      "iteration 2270, training loss: 0.54, accuracy: 69.6\n",
      "iteration 2280, training loss: 0.50, accuracy: 61.2\n",
      "iteration 2290, training loss: 0.53, accuracy: 69.6\n",
      "iteration 2300, training loss: 0.54, accuracy: 72.8\n",
      "iteration 2310, training loss: 0.48, accuracy: 62.8\n",
      "iteration 2320, training loss: 0.51, accuracy: 72.0\n",
      "iteration 2330, training loss: 0.48, accuracy: 70.0\n",
      "iteration 2340, training loss: 0.62, accuracy: 74.0\n",
      "iteration 2350, training loss: 0.59, accuracy: 67.2\n",
      "iteration 2360, training loss: 0.48, accuracy: 67.6\n",
      "iteration 2370, training loss: 0.62, accuracy: 66.4\n",
      "iteration 2380, training loss: 0.52, accuracy: 65.6\n",
      "iteration 2390, training loss: 0.67, accuracy: 64.8\n",
      "iteration 2400, training loss: 0.48, accuracy: 70.8\n",
      "iteration 2410, training loss: 0.46, accuracy: 71.2\n",
      "iteration 2420, training loss: 0.58, accuracy: 66.0\n",
      "iteration 2430, training loss: 0.64, accuracy: 70.0\n",
      "iteration 2440, training loss: 0.52, accuracy: 68.8\n",
      "iteration 2450, training loss: 0.74, accuracy: 65.6\n",
      "iteration 2460, training loss: 0.53, accuracy: 71.2\n",
      "iteration 2470, training loss: 0.51, accuracy: 66.8\n",
      "iteration 2480, training loss: 0.44, accuracy: 69.2\n",
      "iteration 2490, training loss: 0.45, accuracy: 68.0\n",
      "iteration 2500, training loss: 0.52, accuracy: 59.6\n",
      "iteration 2510, training loss: 0.55, accuracy: 70.8\n",
      "iteration 2520, training loss: 0.58, accuracy: 62.8\n",
      "iteration 2530, training loss: 0.60, accuracy: 64.4\n",
      "iteration 2540, training loss: 0.61, accuracy: 71.2\n",
      "iteration 2550, training loss: 0.48, accuracy: 69.2\n",
      "iteration 2560, training loss: 0.55, accuracy: 72.4\n",
      "iteration 2570, training loss: 0.73, accuracy: 66.0\n",
      "iteration 2580, training loss: 0.49, accuracy: 66.8\n",
      "iteration 2590, training loss: 0.57, accuracy: 77.2\n",
      "iteration 2600, training loss: 0.45, accuracy: 73.6\n",
      "iteration 2610, training loss: 0.56, accuracy: 73.6\n",
      "iteration 2620, training loss: 0.42, accuracy: 64.4\n",
      "iteration 2630, training loss: 0.58, accuracy: 76.4\n",
      "iteration 2640, training loss: 0.51, accuracy: 74.8\n",
      "iteration 2650, training loss: 0.60, accuracy: 65.6\n",
      "iteration 2660, training loss: 0.54, accuracy: 67.6\n",
      "iteration 2670, training loss: 0.58, accuracy: 68.4\n",
      "iteration 2680, training loss: 0.47, accuracy: 70.0\n",
      "iteration 2690, training loss: 0.61, accuracy: 66.8\n",
      "iteration 2700, training loss: 0.44, accuracy: 66.8\n",
      "iteration 2710, training loss: 0.42, accuracy: 70.0\n",
      "iteration 2720, training loss: 0.57, accuracy: 71.2\n",
      "iteration 2730, training loss: 0.43, accuracy: 62.4\n",
      "iteration 2740, training loss: 0.55, accuracy: 68.8\n",
      "iteration 2750, training loss: 0.62, accuracy: 65.6\n",
      "iteration 2760, training loss: 0.46, accuracy: 67.6\n",
      "iteration 2770, training loss: 0.52, accuracy: 68.4\n",
      "iteration 2780, training loss: 0.50, accuracy: 69.2\n",
      "iteration 2790, training loss: 0.50, accuracy: 76.4\n",
      "iteration 2800, training loss: 0.47, accuracy: 67.2\n",
      "iteration 2810, training loss: 0.42, accuracy: 61.2\n",
      "iteration 2820, training loss: 0.46, accuracy: 68.8\n",
      "iteration 2830, training loss: 0.46, accuracy: 67.2\n",
      "iteration 2840, training loss: 0.42, accuracy: 68.8\n",
      "iteration 2850, training loss: 0.47, accuracy: 70.4\n",
      "iteration 2860, training loss: 0.50, accuracy: 69.6\n",
      "iteration 2870, training loss: 0.40, accuracy: 68.0\n",
      "iteration 2880, training loss: 0.40, accuracy: 67.2\n",
      "iteration 2890, training loss: 0.49, accuracy: 72.4\n",
      "iteration 2900, training loss: 0.44, accuracy: 67.6\n",
      "iteration 2910, training loss: 0.56, accuracy: 70.8\n",
      "iteration 2920, training loss: 0.63, accuracy: 67.2\n",
      "iteration 2930, training loss: 0.41, accuracy: 71.6\n",
      "iteration 2940, training loss: 0.44, accuracy: 70.4\n",
      "iteration 2950, training loss: 0.46, accuracy: 64.8\n",
      "iteration 2960, training loss: 0.45, accuracy: 72.0\n",
      "iteration 2970, training loss: 0.42, accuracy: 72.0\n",
      "iteration 2980, training loss: 0.43, accuracy: 71.2\n",
      "iteration 2990, training loss: 0.51, accuracy: 69.6\n",
      "iteration 3000, training loss: 0.47, accuracy: 73.2\n",
      "iteration 3010, training loss: 0.46, accuracy: 65.2\n",
      "iteration 3020, training loss: 0.39, accuracy: 76.0\n",
      "iteration 3030, training loss: 0.51, accuracy: 71.6\n",
      "iteration 3040, training loss: 0.44, accuracy: 70.8\n",
      "iteration 3050, training loss: 0.64, accuracy: 69.6\n",
      "iteration 3060, training loss: 0.58, accuracy: 72.4\n",
      "iteration 3070, training loss: 0.39, accuracy: 66.0\n",
      "iteration 3080, training loss: 0.53, accuracy: 70.4\n",
      "iteration 3090, training loss: 0.45, accuracy: 75.2\n",
      "iteration 3100, training loss: 0.50, accuracy: 69.6\n",
      "iteration 3110, training loss: 0.38, accuracy: 68.8\n",
      "iteration 3120, training loss: 0.41, accuracy: 76.8\n",
      "iteration 3130, training loss: 0.52, accuracy: 68.4\n",
      "iteration 3140, training loss: 0.53, accuracy: 65.2\n",
      "iteration 3150, training loss: 0.52, accuracy: 68.0\n",
      "iteration 3160, training loss: 0.40, accuracy: 70.4\n",
      "iteration 3170, training loss: 0.64, accuracy: 69.6\n",
      "iteration 3180, training loss: 0.42, accuracy: 66.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3190, training loss: 0.62, accuracy: 72.4\n",
      "iteration 3200, training loss: 0.38, accuracy: 70.0\n",
      "iteration 3210, training loss: 0.46, accuracy: 64.4\n",
      "iteration 3220, training loss: 0.34, accuracy: 67.6\n",
      "iteration 3230, training loss: 0.38, accuracy: 76.8\n",
      "iteration 3240, training loss: 0.40, accuracy: 68.4\n",
      "iteration 3250, training loss: 0.51, accuracy: 69.6\n",
      "iteration 3260, training loss: 0.43, accuracy: 65.6\n",
      "iteration 3270, training loss: 0.37, accuracy: 75.2\n",
      "iteration 3280, training loss: 0.36, accuracy: 62.0\n",
      "iteration 3290, training loss: 0.38, accuracy: 66.8\n",
      "iteration 3300, training loss: 0.36, accuracy: 69.6\n",
      "iteration 3310, training loss: 0.55, accuracy: 64.4\n",
      "iteration 3320, training loss: 0.37, accuracy: 71.6\n",
      "iteration 3330, training loss: 0.62, accuracy: 69.6\n",
      "iteration 3340, training loss: 0.54, accuracy: 73.2\n",
      "iteration 3350, training loss: 0.44, accuracy: 74.4\n",
      "iteration 3360, training loss: 0.38, accuracy: 69.6\n",
      "iteration 3370, training loss: 0.43, accuracy: 70.8\n",
      "iteration 3380, training loss: 0.46, accuracy: 72.8\n",
      "iteration 3390, training loss: 0.58, accuracy: 67.6\n",
      "iteration 3400, training loss: 0.35, accuracy: 72.8\n",
      "iteration 3410, training loss: 0.36, accuracy: 74.0\n",
      "iteration 3420, training loss: 0.39, accuracy: 68.8\n",
      "iteration 3430, training loss: 0.40, accuracy: 68.0\n",
      "iteration 3440, training loss: 0.48, accuracy: 74.8\n",
      "iteration 3450, training loss: 0.43, accuracy: 69.6\n",
      "iteration 3460, training loss: 0.34, accuracy: 70.8\n",
      "iteration 3470, training loss: 0.35, accuracy: 69.6\n",
      "iteration 3480, training loss: 0.39, accuracy: 67.2\n",
      "iteration 3490, training loss: 0.37, accuracy: 74.0\n",
      "iteration 3500, training loss: 0.40, accuracy: 73.6\n",
      "iteration 3510, training loss: 0.32, accuracy: 74.8\n",
      "iteration 3520, training loss: 0.45, accuracy: 71.2\n",
      "iteration 3530, training loss: 0.54, accuracy: 70.8\n",
      "iteration 3540, training loss: 0.49, accuracy: 66.8\n",
      "iteration 3550, training loss: 0.37, accuracy: 67.2\n",
      "iteration 3560, training loss: 0.37, accuracy: 73.6\n",
      "iteration 3570, training loss: 0.34, accuracy: 71.2\n",
      "iteration 3580, training loss: 0.38, accuracy: 74.0\n",
      "iteration 3590, training loss: 0.52, accuracy: 71.2\n",
      "iteration 3600, training loss: 0.35, accuracy: 64.8\n",
      "iteration 3610, training loss: 0.42, accuracy: 71.2\n",
      "iteration 3620, training loss: 0.35, accuracy: 66.0\n",
      "iteration 3630, training loss: 0.46, accuracy: 73.2\n",
      "iteration 3640, training loss: 0.37, accuracy: 70.4\n",
      "iteration 3650, training loss: 0.47, accuracy: 74.4\n",
      "iteration 3660, training loss: 0.43, accuracy: 73.2\n",
      "iteration 3670, training loss: 0.33, accuracy: 67.6\n",
      "iteration 3680, training loss: 0.58, accuracy: 73.2\n",
      "iteration 3690, training loss: 0.51, accuracy: 66.8\n",
      "iteration 3700, training loss: 0.37, accuracy: 72.0\n",
      "iteration 3710, training loss: 0.40, accuracy: 68.4\n",
      "iteration 3720, training loss: 0.47, accuracy: 72.8\n",
      "iteration 3730, training loss: 0.43, accuracy: 69.2\n",
      "iteration 3740, training loss: 0.34, accuracy: 70.8\n",
      "iteration 3750, training loss: 0.37, accuracy: 74.8\n",
      "iteration 3760, training loss: 0.36, accuracy: 70.8\n",
      "iteration 3770, training loss: 0.38, accuracy: 74.4\n",
      "iteration 3780, training loss: 0.36, accuracy: 70.0\n",
      "iteration 3790, training loss: 0.43, accuracy: 70.0\n",
      "iteration 3800, training loss: 0.44, accuracy: 72.0\n",
      "iteration 3810, training loss: 0.42, accuracy: 73.2\n",
      "iteration 3820, training loss: 0.49, accuracy: 70.4\n",
      "iteration 3830, training loss: 0.48, accuracy: 76.4\n",
      "iteration 3840, training loss: 0.50, accuracy: 72.4\n",
      "iteration 3850, training loss: 0.31, accuracy: 65.6\n",
      "iteration 3860, training loss: 0.32, accuracy: 75.2\n",
      "iteration 3870, training loss: 0.49, accuracy: 69.2\n",
      "iteration 3880, training loss: 0.38, accuracy: 70.4\n",
      "iteration 3890, training loss: 0.38, accuracy: 74.0\n",
      "iteration 3900, training loss: 0.36, accuracy: 72.4\n",
      "iteration 3910, training loss: 0.43, accuracy: 73.2\n",
      "iteration 3920, training loss: 0.33, accuracy: 63.6\n",
      "iteration 3930, training loss: 0.45, accuracy: 65.6\n",
      "iteration 3940, training loss: 0.34, accuracy: 65.2\n",
      "iteration 3950, training loss: 0.39, accuracy: 70.0\n",
      "iteration 3960, training loss: 0.38, accuracy: 65.6\n",
      "iteration 3970, training loss: 0.50, accuracy: 71.2\n",
      "iteration 3980, training loss: 0.34, accuracy: 74.4\n",
      "iteration 3990, training loss: 0.47, accuracy: 71.6\n",
      "iteration 4000, training loss: 0.37, accuracy: 72.0\n",
      "iteration 4010, training loss: 0.32, accuracy: 74.0\n",
      "iteration 4020, training loss: 0.36, accuracy: 74.4\n",
      "iteration 4030, training loss: 0.32, accuracy: 70.8\n",
      "iteration 4040, training loss: 0.30, accuracy: 70.4\n",
      "iteration 4050, training loss: 0.46, accuracy: 72.8\n",
      "iteration 4060, training loss: 0.42, accuracy: 72.4\n",
      "iteration 4070, training loss: 0.31, accuracy: 73.6\n",
      "iteration 4080, training loss: 0.42, accuracy: 74.8\n",
      "iteration 4090, training loss: 0.43, accuracy: 70.4\n",
      "iteration 4100, training loss: 0.48, accuracy: 68.0\n",
      "iteration 4110, training loss: 0.34, accuracy: 69.6\n",
      "iteration 4120, training loss: 0.36, accuracy: 63.2\n",
      "iteration 4130, training loss: 0.34, accuracy: 67.2\n",
      "iteration 4140, training loss: 0.50, accuracy: 75.2\n",
      "iteration 4150, training loss: 0.31, accuracy: 76.8\n",
      "iteration 4160, training loss: 0.42, accuracy: 70.0\n",
      "iteration 4170, training loss: 0.41, accuracy: 71.6\n",
      "iteration 4180, training loss: 0.37, accuracy: 72.4\n",
      "iteration 4190, training loss: 0.58, accuracy: 78.4\n",
      "iteration 4200, training loss: 0.37, accuracy: 72.4\n",
      "iteration 4210, training loss: 0.27, accuracy: 71.6\n",
      "iteration 4220, training loss: 0.30, accuracy: 76.0\n",
      "iteration 4230, training loss: 0.29, accuracy: 75.2\n",
      "iteration 4240, training loss: 0.39, accuracy: 71.6\n",
      "iteration 4250, training loss: 0.39, accuracy: 72.8\n",
      "iteration 4260, training loss: 0.63, accuracy: 66.8\n",
      "iteration 4270, training loss: 0.35, accuracy: 69.2\n",
      "iteration 4280, training loss: 0.34, accuracy: 69.6\n",
      "iteration 4290, training loss: 0.28, accuracy: 76.8\n",
      "iteration 4300, training loss: 0.40, accuracy: 69.6\n",
      "iteration 4310, training loss: 0.34, accuracy: 71.6\n",
      "iteration 4320, training loss: 0.44, accuracy: 72.4\n",
      "iteration 4330, training loss: 0.29, accuracy: 74.0\n",
      "iteration 4340, training loss: 0.37, accuracy: 73.6\n",
      "iteration 4350, training loss: 0.32, accuracy: 76.4\n",
      "iteration 4360, training loss: 0.33, accuracy: 76.0\n",
      "iteration 4370, training loss: 0.29, accuracy: 79.2\n",
      "iteration 4380, training loss: 0.38, accuracy: 71.2\n",
      "iteration 4390, training loss: 0.34, accuracy: 70.8\n",
      "iteration 4400, training loss: 0.33, accuracy: 77.2\n",
      "iteration 4410, training loss: 0.27, accuracy: 70.8\n",
      "iteration 4420, training loss: 0.57, accuracy: 76.4\n",
      "iteration 4430, training loss: 0.30, accuracy: 79.2\n",
      "iteration 4440, training loss: 0.33, accuracy: 69.6\n",
      "iteration 4450, training loss: 0.36, accuracy: 67.2\n",
      "iteration 4460, training loss: 0.44, accuracy: 76.8\n",
      "iteration 4470, training loss: 0.44, accuracy: 72.4\n",
      "iteration 4480, training loss: 0.29, accuracy: 72.0\n",
      "iteration 4490, training loss: 0.36, accuracy: 77.2\n",
      "iteration 4500, training loss: 0.35, accuracy: 75.6\n",
      "iteration 4510, training loss: 0.48, accuracy: 76.4\n",
      "iteration 4520, training loss: 0.33, accuracy: 70.0\n",
      "iteration 4530, training loss: 0.50, accuracy: 76.4\n",
      "iteration 4540, training loss: 0.33, accuracy: 75.6\n",
      "iteration 4550, training loss: 0.37, accuracy: 72.4\n",
      "iteration 4560, training loss: 0.44, accuracy: 75.6\n",
      "iteration 4570, training loss: 0.38, accuracy: 75.2\n",
      "iteration 4580, training loss: 0.35, accuracy: 74.8\n",
      "iteration 4590, training loss: 0.29, accuracy: 73.6\n",
      "iteration 4600, training loss: 0.40, accuracy: 72.8\n",
      "iteration 4610, training loss: 0.44, accuracy: 76.8\n",
      "iteration 4620, training loss: 0.37, accuracy: 69.2\n",
      "iteration 4630, training loss: 0.36, accuracy: 69.6\n",
      "iteration 4640, training loss: 0.31, accuracy: 72.0\n",
      "iteration 4650, training loss: 0.42, accuracy: 78.0\n",
      "iteration 4660, training loss: 0.38, accuracy: 73.6\n",
      "iteration 4670, training loss: 0.42, accuracy: 80.4\n",
      "iteration 4680, training loss: 0.31, accuracy: 68.4\n",
      "iteration 4690, training loss: 0.33, accuracy: 76.4\n",
      "iteration 4700, training loss: 0.37, accuracy: 76.8\n",
      "iteration 4710, training loss: 0.43, accuracy: 72.0\n",
      "iteration 4720, training loss: 0.47, accuracy: 76.8\n",
      "iteration 4730, training loss: 0.32, accuracy: 73.2\n",
      "iteration 4740, training loss: 0.36, accuracy: 76.4\n",
      "iteration 4750, training loss: 0.38, accuracy: 75.2\n",
      "iteration 4760, training loss: 0.30, accuracy: 68.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4770, training loss: 0.31, accuracy: 69.2\n",
      "iteration 4780, training loss: 0.30, accuracy: 78.8\n",
      "iteration 4790, training loss: 0.39, accuracy: 76.4\n",
      "iteration 4800, training loss: 0.40, accuracy: 72.8\n",
      "iteration 4810, training loss: 0.38, accuracy: 71.6\n",
      "iteration 4820, training loss: 0.31, accuracy: 71.6\n",
      "iteration 4830, training loss: 0.28, accuracy: 77.6\n",
      "iteration 4840, training loss: 0.32, accuracy: 76.0\n",
      "iteration 4850, training loss: 0.27, accuracy: 71.2\n",
      "iteration 4860, training loss: 0.28, accuracy: 75.6\n",
      "iteration 4870, training loss: 0.26, accuracy: 75.6\n",
      "iteration 4880, training loss: 0.25, accuracy: 68.8\n",
      "iteration 4890, training loss: 0.33, accuracy: 74.0\n",
      "iteration 4900, training loss: 0.27, accuracy: 80.8\n",
      "iteration 4910, training loss: 0.34, accuracy: 79.6\n",
      "iteration 4920, training loss: 0.29, accuracy: 77.2\n",
      "iteration 4930, training loss: 0.32, accuracy: 75.6\n",
      "iteration 4940, training loss: 0.47, accuracy: 75.2\n",
      "iteration 4950, training loss: 0.51, accuracy: 70.0\n",
      "iteration 4960, training loss: 0.40, accuracy: 75.6\n",
      "iteration 4970, training loss: 0.26, accuracy: 72.4\n",
      "iteration 4980, training loss: 0.25, accuracy: 76.4\n",
      "iteration 4990, training loss: 0.28, accuracy: 77.2\n",
      "iteration 5000, training loss: 0.30, accuracy: 74.8\n",
      "iteration 5010, training loss: 0.31, accuracy: 73.2\n",
      "iteration 5020, training loss: 0.35, accuracy: 72.8\n",
      "iteration 5030, training loss: 0.32, accuracy: 74.8\n",
      "iteration 5040, training loss: 0.30, accuracy: 71.2\n",
      "iteration 5050, training loss: 0.34, accuracy: 67.6\n",
      "iteration 5060, training loss: 0.36, accuracy: 72.4\n",
      "iteration 5070, training loss: 0.29, accuracy: 76.4\n",
      "iteration 5080, training loss: 0.29, accuracy: 71.6\n",
      "iteration 5090, training loss: 0.34, accuracy: 77.6\n",
      "iteration 5100, training loss: 0.31, accuracy: 72.0\n",
      "iteration 5110, training loss: 0.32, accuracy: 79.2\n",
      "iteration 5120, training loss: 0.23, accuracy: 69.2\n",
      "iteration 5130, training loss: 0.36, accuracy: 70.4\n",
      "iteration 5140, training loss: 0.26, accuracy: 74.4\n",
      "iteration 5150, training loss: 0.29, accuracy: 69.2\n",
      "iteration 5160, training loss: 0.35, accuracy: 76.4\n",
      "iteration 5170, training loss: 0.30, accuracy: 71.6\n",
      "iteration 5180, training loss: 0.37, accuracy: 79.2\n",
      "iteration 5190, training loss: 0.42, accuracy: 68.0\n",
      "iteration 5200, training loss: 0.38, accuracy: 75.6\n",
      "iteration 5210, training loss: 0.27, accuracy: 75.2\n",
      "iteration 5220, training loss: 0.26, accuracy: 72.4\n",
      "iteration 5230, training loss: 0.36, accuracy: 71.6\n",
      "iteration 5240, training loss: 0.39, accuracy: 78.8\n",
      "iteration 5250, training loss: 0.35, accuracy: 72.8\n",
      "iteration 5260, training loss: 0.30, accuracy: 72.0\n",
      "iteration 5270, training loss: 0.48, accuracy: 79.2\n",
      "iteration 5280, training loss: 0.28, accuracy: 74.0\n",
      "iteration 5290, training loss: 0.29, accuracy: 76.0\n",
      "iteration 5300, training loss: 0.28, accuracy: 73.2\n",
      "iteration 5310, training loss: 0.25, accuracy: 78.8\n",
      "iteration 5320, training loss: 0.34, accuracy: 73.2\n",
      "iteration 5330, training loss: 0.24, accuracy: 76.0\n",
      "iteration 5340, training loss: 0.29, accuracy: 71.2\n",
      "iteration 5350, training loss: 0.27, accuracy: 75.2\n",
      "iteration 5360, training loss: 0.33, accuracy: 77.6\n",
      "iteration 5370, training loss: 0.29, accuracy: 68.8\n",
      "iteration 5380, training loss: 0.26, accuracy: 72.4\n",
      "iteration 5390, training loss: 0.44, accuracy: 75.2\n",
      "iteration 5400, training loss: 0.30, accuracy: 76.8\n",
      "iteration 5410, training loss: 0.38, accuracy: 72.8\n",
      "iteration 5420, training loss: 0.24, accuracy: 72.8\n",
      "iteration 5430, training loss: 0.33, accuracy: 74.4\n",
      "iteration 5440, training loss: 0.27, accuracy: 76.0\n",
      "iteration 5450, training loss: 0.28, accuracy: 73.2\n",
      "iteration 5460, training loss: 0.26, accuracy: 72.0\n",
      "iteration 5470, training loss: 0.29, accuracy: 74.4\n",
      "iteration 5480, training loss: 0.35, accuracy: 73.2\n",
      "iteration 5490, training loss: 0.27, accuracy: 74.0\n",
      "iteration 5500, training loss: 0.24, accuracy: 69.2\n",
      "iteration 5510, training loss: 0.28, accuracy: 72.0\n",
      "iteration 5520, training loss: 0.31, accuracy: 70.0\n",
      "iteration 5530, training loss: 0.29, accuracy: 76.4\n",
      "iteration 5540, training loss: 0.33, accuracy: 72.0\n",
      "iteration 5550, training loss: 0.41, accuracy: 74.0\n",
      "iteration 5560, training loss: 0.26, accuracy: 73.6\n",
      "iteration 5570, training loss: 0.33, accuracy: 75.2\n",
      "iteration 5580, training loss: 0.40, accuracy: 73.6\n",
      "iteration 5590, training loss: 0.35, accuracy: 73.2\n",
      "iteration 5600, training loss: 0.39, accuracy: 75.2\n",
      "iteration 5610, training loss: 0.58, accuracy: 66.4\n",
      "iteration 5620, training loss: 0.24, accuracy: 78.0\n",
      "iteration 5630, training loss: 0.36, accuracy: 74.0\n",
      "iteration 5640, training loss: 0.32, accuracy: 72.8\n",
      "iteration 5650, training loss: 0.26, accuracy: 78.8\n",
      "iteration 5660, training loss: 0.30, accuracy: 78.0\n",
      "iteration 5670, training loss: 0.25, accuracy: 77.6\n",
      "iteration 5680, training loss: 0.28, accuracy: 77.6\n",
      "iteration 5690, training loss: 0.33, accuracy: 77.2\n",
      "iteration 5700, training loss: 0.48, accuracy: 74.8\n",
      "iteration 5710, training loss: 0.37, accuracy: 77.2\n",
      "iteration 5720, training loss: 0.27, accuracy: 72.8\n",
      "iteration 5730, training loss: 0.51, accuracy: 77.2\n",
      "iteration 5740, training loss: 0.29, accuracy: 70.0\n",
      "iteration 5750, training loss: 0.32, accuracy: 76.4\n",
      "iteration 5760, training loss: 0.27, accuracy: 74.8\n",
      "iteration 5770, training loss: 0.37, accuracy: 73.6\n",
      "iteration 5780, training loss: 0.35, accuracy: 79.2\n",
      "iteration 5790, training loss: 0.30, accuracy: 74.8\n",
      "iteration 5800, training loss: 0.31, accuracy: 73.6\n",
      "iteration 5810, training loss: 0.35, accuracy: 73.6\n",
      "iteration 5820, training loss: 0.24, accuracy: 78.4\n",
      "iteration 5830, training loss: 0.24, accuracy: 76.4\n",
      "iteration 5840, training loss: 0.35, accuracy: 74.4\n",
      "iteration 5850, training loss: 0.26, accuracy: 76.0\n",
      "iteration 5860, training loss: 0.29, accuracy: 78.8\n",
      "iteration 5870, training loss: 0.33, accuracy: 72.0\n",
      "iteration 5880, training loss: 0.34, accuracy: 76.8\n",
      "iteration 5890, training loss: 0.38, accuracy: 78.4\n",
      "iteration 5900, training loss: 0.36, accuracy: 78.0\n",
      "iteration 5910, training loss: 0.34, accuracy: 73.6\n",
      "iteration 5920, training loss: 0.24, accuracy: 73.6\n",
      "iteration 5930, training loss: 0.21, accuracy: 70.0\n",
      "iteration 5940, training loss: 0.34, accuracy: 77.2\n",
      "iteration 5950, training loss: 0.26, accuracy: 79.6\n",
      "iteration 5960, training loss: 0.33, accuracy: 74.8\n",
      "iteration 5970, training loss: 0.46, accuracy: 72.8\n",
      "iteration 5980, training loss: 0.24, accuracy: 70.4\n",
      "iteration 5990, training loss: 0.43, accuracy: 76.4\n",
      "iteration 6000, training loss: 0.29, accuracy: 78.0\n",
      "iteration 6010, training loss: 0.30, accuracy: 80.8\n",
      "iteration 6020, training loss: 0.27, accuracy: 80.0\n",
      "iteration 6030, training loss: 0.26, accuracy: 74.4\n",
      "iteration 6040, training loss: 0.31, accuracy: 72.8\n",
      "iteration 6050, training loss: 0.39, accuracy: 74.4\n",
      "iteration 6060, training loss: 0.32, accuracy: 78.8\n",
      "iteration 6070, training loss: 0.31, accuracy: 77.6\n",
      "iteration 6080, training loss: 0.30, accuracy: 73.6\n",
      "iteration 6090, training loss: 0.37, accuracy: 70.0\n",
      "iteration 6100, training loss: 0.25, accuracy: 78.8\n",
      "iteration 6110, training loss: 0.23, accuracy: 74.0\n",
      "iteration 6120, training loss: 0.31, accuracy: 71.6\n",
      "iteration 6130, training loss: 0.26, accuracy: 74.8\n",
      "iteration 6140, training loss: 0.28, accuracy: 75.6\n",
      "iteration 6150, training loss: 0.20, accuracy: 78.0\n",
      "iteration 6160, training loss: 0.30, accuracy: 75.2\n",
      "iteration 6170, training loss: 0.33, accuracy: 76.0\n",
      "iteration 6180, training loss: 0.27, accuracy: 76.8\n",
      "iteration 6190, training loss: 0.44, accuracy: 77.2\n",
      "iteration 6200, training loss: 0.30, accuracy: 73.2\n",
      "iteration 6210, training loss: 0.42, accuracy: 65.6\n",
      "iteration 6220, training loss: 0.34, accuracy: 76.0\n",
      "iteration 6230, training loss: 0.29, accuracy: 73.2\n",
      "iteration 6240, training loss: 0.33, accuracy: 64.8\n",
      "iteration 6250, training loss: 0.30, accuracy: 66.4\n",
      "iteration 6260, training loss: 0.23, accuracy: 71.6\n",
      "iteration 6270, training loss: 0.30, accuracy: 72.0\n",
      "iteration 6280, training loss: 0.34, accuracy: 79.6\n",
      "iteration 6290, training loss: 0.42, accuracy: 78.8\n",
      "iteration 6300, training loss: 0.25, accuracy: 73.6\n",
      "iteration 6310, training loss: 0.41, accuracy: 80.4\n",
      "iteration 6320, training loss: 0.34, accuracy: 72.0\n",
      "iteration 6330, training loss: 0.27, accuracy: 73.6\n",
      "iteration 6340, training loss: 0.30, accuracy: 71.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6350, training loss: 0.24, accuracy: 77.6\n",
      "iteration 6360, training loss: 0.25, accuracy: 76.8\n",
      "iteration 6370, training loss: 0.29, accuracy: 75.6\n",
      "iteration 6380, training loss: 0.26, accuracy: 74.4\n",
      "iteration 6390, training loss: 0.23, accuracy: 74.4\n",
      "iteration 6400, training loss: 0.21, accuracy: 77.2\n",
      "iteration 6410, training loss: 0.30, accuracy: 75.2\n",
      "iteration 6420, training loss: 0.25, accuracy: 78.8\n",
      "iteration 6430, training loss: 0.22, accuracy: 70.4\n",
      "iteration 6440, training loss: 0.30, accuracy: 71.6\n",
      "iteration 6450, training loss: 0.29, accuracy: 76.4\n",
      "iteration 6460, training loss: 0.35, accuracy: 76.8\n",
      "iteration 6470, training loss: 0.29, accuracy: 72.4\n",
      "iteration 6480, training loss: 0.31, accuracy: 70.0\n",
      "iteration 6490, training loss: 0.27, accuracy: 68.4\n",
      "iteration 6500, training loss: 0.45, accuracy: 77.6\n",
      "iteration 6510, training loss: 0.44, accuracy: 76.0\n",
      "iteration 6520, training loss: 0.26, accuracy: 75.2\n",
      "iteration 6530, training loss: 0.28, accuracy: 77.6\n",
      "iteration 6540, training loss: 0.27, accuracy: 74.4\n",
      "iteration 6550, training loss: 0.22, accuracy: 70.4\n",
      "iteration 6560, training loss: 0.30, accuracy: 74.8\n",
      "iteration 6570, training loss: 0.26, accuracy: 76.4\n",
      "iteration 6580, training loss: 0.24, accuracy: 76.8\n",
      "iteration 6590, training loss: 0.33, accuracy: 71.2\n",
      "iteration 6600, training loss: 0.30, accuracy: 76.8\n",
      "iteration 6610, training loss: 0.31, accuracy: 73.2\n",
      "iteration 6620, training loss: 0.34, accuracy: 75.2\n",
      "iteration 6630, training loss: 0.31, accuracy: 81.2\n",
      "iteration 6640, training loss: 0.24, accuracy: 74.8\n",
      "iteration 6650, training loss: 0.23, accuracy: 76.4\n",
      "iteration 6660, training loss: 0.22, accuracy: 72.8\n",
      "iteration 6670, training loss: 0.24, accuracy: 75.2\n",
      "iteration 6680, training loss: 0.30, accuracy: 70.0\n",
      "iteration 6690, training loss: 0.24, accuracy: 78.8\n",
      "iteration 6700, training loss: 0.24, accuracy: 76.4\n",
      "iteration 6710, training loss: 0.24, accuracy: 72.0\n",
      "iteration 6720, training loss: 0.23, accuracy: 74.0\n",
      "iteration 6730, training loss: 0.21, accuracy: 69.6\n",
      "iteration 6740, training loss: 0.25, accuracy: 84.0\n",
      "iteration 6750, training loss: 0.24, accuracy: 74.8\n",
      "iteration 6760, training loss: 0.26, accuracy: 70.8\n",
      "iteration 6770, training loss: 0.30, accuracy: 80.8\n",
      "iteration 6780, training loss: 0.40, accuracy: 75.2\n",
      "iteration 6790, training loss: 0.29, accuracy: 78.4\n",
      "iteration 6800, training loss: 0.38, accuracy: 77.2\n",
      "iteration 6810, training loss: 0.41, accuracy: 74.0\n",
      "iteration 6820, training loss: 0.28, accuracy: 74.8\n",
      "iteration 6830, training loss: 0.29, accuracy: 72.4\n",
      "iteration 6840, training loss: 0.40, accuracy: 76.4\n",
      "iteration 6850, training loss: 0.24, accuracy: 77.6\n",
      "iteration 6860, training loss: 0.19, accuracy: 76.0\n",
      "iteration 6870, training loss: 0.22, accuracy: 74.4\n",
      "iteration 6880, training loss: 0.23, accuracy: 76.0\n",
      "iteration 6890, training loss: 0.23, accuracy: 80.0\n",
      "iteration 6900, training loss: 0.23, accuracy: 68.4\n",
      "iteration 6910, training loss: 0.28, accuracy: 78.0\n",
      "iteration 6920, training loss: 0.25, accuracy: 75.2\n",
      "iteration 6930, training loss: 0.25, accuracy: 81.2\n",
      "iteration 6940, training loss: 0.28, accuracy: 79.2\n",
      "iteration 6950, training loss: 0.29, accuracy: 78.0\n",
      "iteration 6960, training loss: 0.38, accuracy: 72.0\n",
      "iteration 6970, training loss: 0.31, accuracy: 74.0\n",
      "iteration 6980, training loss: 0.29, accuracy: 70.8\n",
      "iteration 6990, training loss: 0.21, accuracy: 80.4\n",
      "iteration 7000, training loss: 0.19, accuracy: 76.8\n",
      "iteration 7010, training loss: 0.23, accuracy: 73.6\n",
      "iteration 7020, training loss: 0.27, accuracy: 76.4\n",
      "iteration 7030, training loss: 0.37, accuracy: 72.8\n",
      "iteration 7040, training loss: 0.25, accuracy: 75.6\n",
      "iteration 7050, training loss: 0.29, accuracy: 76.0\n",
      "iteration 7060, training loss: 0.45, accuracy: 70.8\n",
      "iteration 7070, training loss: 0.24, accuracy: 78.0\n",
      "iteration 7080, training loss: 0.32, accuracy: 71.2\n",
      "iteration 7090, training loss: 0.32, accuracy: 77.2\n",
      "iteration 7100, training loss: 0.20, accuracy: 76.4\n",
      "iteration 7110, training loss: 0.21, accuracy: 67.2\n",
      "iteration 7120, training loss: 0.26, accuracy: 72.0\n",
      "iteration 7130, training loss: 0.22, accuracy: 76.8\n",
      "iteration 7140, training loss: 0.23, accuracy: 77.2\n",
      "iteration 7150, training loss: 0.22, accuracy: 75.6\n",
      "iteration 7160, training loss: 0.24, accuracy: 76.0\n",
      "iteration 7170, training loss: 0.32, accuracy: 73.6\n",
      "iteration 7180, training loss: 0.26, accuracy: 72.4\n",
      "iteration 7190, training loss: 0.27, accuracy: 79.2\n",
      "iteration 7200, training loss: 0.37, accuracy: 74.4\n",
      "iteration 7210, training loss: 0.25, accuracy: 77.6\n",
      "iteration 7220, training loss: 0.25, accuracy: 75.6\n",
      "iteration 7230, training loss: 0.31, accuracy: 75.6\n",
      "iteration 7240, training loss: 0.29, accuracy: 78.0\n",
      "iteration 7250, training loss: 0.26, accuracy: 72.8\n",
      "iteration 7260, training loss: 0.21, accuracy: 72.4\n",
      "iteration 7270, training loss: 0.22, accuracy: 75.2\n",
      "iteration 7280, training loss: 0.27, accuracy: 76.8\n",
      "iteration 7290, training loss: 0.24, accuracy: 76.4\n",
      "iteration 7300, training loss: 0.37, accuracy: 76.0\n",
      "iteration 7310, training loss: 0.34, accuracy: 80.4\n",
      "iteration 7320, training loss: 0.34, accuracy: 77.2\n",
      "iteration 7330, training loss: 0.22, accuracy: 76.0\n",
      "iteration 7340, training loss: 0.31, accuracy: 82.4\n",
      "iteration 7350, training loss: 0.27, accuracy: 76.8\n",
      "iteration 7360, training loss: 0.39, accuracy: 72.4\n",
      "iteration 7370, training loss: 0.39, accuracy: 74.8\n",
      "iteration 7380, training loss: 0.22, accuracy: 74.0\n",
      "iteration 7390, training loss: 0.36, accuracy: 79.6\n",
      "iteration 7400, training loss: 0.31, accuracy: 79.6\n",
      "iteration 7410, training loss: 0.24, accuracy: 72.8\n",
      "iteration 7420, training loss: 0.26, accuracy: 76.0\n",
      "iteration 7430, training loss: 0.33, accuracy: 76.4\n",
      "iteration 7440, training loss: 0.40, accuracy: 73.6\n",
      "iteration 7450, training loss: 0.35, accuracy: 75.2\n",
      "iteration 7460, training loss: 0.35, accuracy: 75.2\n",
      "iteration 7470, training loss: 0.19, accuracy: 75.2\n",
      "iteration 7480, training loss: 0.20, accuracy: 76.4\n",
      "iteration 7490, training loss: 0.28, accuracy: 78.0\n",
      "iteration 7500, training loss: 0.30, accuracy: 75.6\n",
      "iteration 7510, training loss: 0.20, accuracy: 75.2\n",
      "iteration 7520, training loss: 0.33, accuracy: 75.6\n",
      "iteration 7530, training loss: 0.26, accuracy: 68.8\n",
      "iteration 7540, training loss: 0.27, accuracy: 70.0\n",
      "iteration 7550, training loss: 0.21, accuracy: 80.4\n",
      "iteration 7560, training loss: 0.22, accuracy: 71.6\n",
      "iteration 7570, training loss: 0.25, accuracy: 76.8\n",
      "iteration 7580, training loss: 0.23, accuracy: 74.8\n",
      "iteration 7590, training loss: 0.32, accuracy: 74.4\n",
      "iteration 7600, training loss: 0.36, accuracy: 67.6\n",
      "iteration 7610, training loss: 0.22, accuracy: 76.8\n",
      "iteration 7620, training loss: 0.20, accuracy: 82.4\n",
      "iteration 7630, training loss: 0.41, accuracy: 74.4\n",
      "iteration 7640, training loss: 0.28, accuracy: 82.8\n",
      "iteration 7650, training loss: 0.32, accuracy: 78.8\n",
      "iteration 7660, training loss: 0.25, accuracy: 76.8\n",
      "iteration 7670, training loss: 0.28, accuracy: 77.2\n",
      "iteration 7680, training loss: 0.22, accuracy: 80.0\n",
      "iteration 7690, training loss: 0.24, accuracy: 73.6\n",
      "iteration 7700, training loss: 0.23, accuracy: 74.4\n",
      "iteration 7710, training loss: 0.26, accuracy: 75.2\n",
      "iteration 7720, training loss: 0.22, accuracy: 79.6\n",
      "iteration 7730, training loss: 0.26, accuracy: 77.6\n",
      "iteration 7740, training loss: 0.20, accuracy: 81.2\n",
      "iteration 7750, training loss: 0.22, accuracy: 78.0\n",
      "iteration 7760, training loss: 0.26, accuracy: 78.0\n",
      "iteration 7770, training loss: 0.26, accuracy: 74.8\n",
      "iteration 7780, training loss: 0.22, accuracy: 70.4\n",
      "iteration 7790, training loss: 0.23, accuracy: 72.4\n",
      "iteration 7800, training loss: 0.23, accuracy: 73.2\n",
      "iteration 7810, training loss: 0.22, accuracy: 75.6\n",
      "iteration 7820, training loss: 0.30, accuracy: 72.8\n",
      "iteration 7830, training loss: 0.27, accuracy: 81.2\n",
      "iteration 7840, training loss: 0.30, accuracy: 74.4\n",
      "iteration 7850, training loss: 0.25, accuracy: 78.0\n",
      "iteration 7860, training loss: 0.43, accuracy: 77.2\n",
      "iteration 7870, training loss: 0.20, accuracy: 74.8\n",
      "iteration 7880, training loss: 0.35, accuracy: 82.8\n",
      "iteration 7890, training loss: 0.55, accuracy: 72.0\n",
      "iteration 7900, training loss: 0.33, accuracy: 74.8\n",
      "iteration 7910, training loss: 0.26, accuracy: 78.0\n",
      "iteration 7920, training loss: 0.25, accuracy: 80.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7930, training loss: 0.44, accuracy: 74.8\n",
      "iteration 7940, training loss: 0.21, accuracy: 78.4\n",
      "iteration 7950, training loss: 0.26, accuracy: 79.6\n",
      "iteration 7960, training loss: 0.23, accuracy: 78.4\n",
      "iteration 7970, training loss: 0.20, accuracy: 78.0\n",
      "iteration 7980, training loss: 0.22, accuracy: 75.6\n",
      "iteration 7990, training loss: 0.31, accuracy: 78.4\n",
      "iteration 8000, training loss: 0.25, accuracy: 82.4\n",
      "iteration 8010, training loss: 0.26, accuracy: 79.6\n",
      "iteration 8020, training loss: 0.21, accuracy: 78.0\n",
      "iteration 8030, training loss: 0.38, accuracy: 79.6\n",
      "iteration 8040, training loss: 0.33, accuracy: 73.2\n",
      "iteration 8050, training loss: 0.25, accuracy: 70.0\n",
      "iteration 8060, training loss: 0.25, accuracy: 78.0\n",
      "iteration 8070, training loss: 0.30, accuracy: 76.4\n",
      "iteration 8080, training loss: 0.27, accuracy: 75.2\n",
      "iteration 8090, training loss: 0.30, accuracy: 74.4\n",
      "iteration 8100, training loss: 0.40, accuracy: 74.0\n",
      "iteration 8110, training loss: 0.22, accuracy: 75.2\n",
      "iteration 8120, training loss: 0.32, accuracy: 76.4\n",
      "iteration 8130, training loss: 0.34, accuracy: 81.6\n",
      "iteration 8140, training loss: 0.20, accuracy: 76.8\n",
      "iteration 8150, training loss: 0.20, accuracy: 76.4\n",
      "iteration 8160, training loss: 0.20, accuracy: 82.0\n",
      "iteration 8170, training loss: 0.33, accuracy: 74.8\n",
      "iteration 8180, training loss: 0.17, accuracy: 75.6\n",
      "iteration 8190, training loss: 0.20, accuracy: 73.2\n",
      "iteration 8200, training loss: 0.33, accuracy: 73.2\n",
      "iteration 8210, training loss: 0.22, accuracy: 79.2\n",
      "iteration 8220, training loss: 0.39, accuracy: 79.2\n",
      "iteration 8230, training loss: 0.23, accuracy: 79.6\n",
      "iteration 8240, training loss: 0.28, accuracy: 72.0\n",
      "iteration 8250, training loss: 0.31, accuracy: 71.2\n",
      "iteration 8260, training loss: 0.19, accuracy: 75.2\n",
      "iteration 8270, training loss: 0.24, accuracy: 76.0\n",
      "iteration 8280, training loss: 0.37, accuracy: 77.2\n",
      "iteration 8290, training loss: 0.22, accuracy: 78.8\n",
      "iteration 8300, training loss: 0.32, accuracy: 70.0\n",
      "iteration 8310, training loss: 0.21, accuracy: 80.8\n",
      "iteration 8320, training loss: 0.19, accuracy: 73.2\n",
      "iteration 8330, training loss: 0.20, accuracy: 72.4\n",
      "iteration 8340, training loss: 0.33, accuracy: 71.6\n",
      "iteration 8350, training loss: 0.24, accuracy: 76.4\n",
      "iteration 8360, training loss: 0.22, accuracy: 78.0\n",
      "iteration 8370, training loss: 0.47, accuracy: 82.8\n",
      "iteration 8380, training loss: 0.19, accuracy: 76.4\n",
      "iteration 8390, training loss: 0.24, accuracy: 78.0\n",
      "iteration 8400, training loss: 0.40, accuracy: 73.2\n",
      "iteration 8410, training loss: 0.24, accuracy: 78.4\n",
      "iteration 8420, training loss: 0.22, accuracy: 77.2\n",
      "iteration 8430, training loss: 0.38, accuracy: 80.0\n",
      "iteration 8440, training loss: 0.23, accuracy: 79.6\n",
      "iteration 8450, training loss: 0.31, accuracy: 76.4\n",
      "iteration 8460, training loss: 0.24, accuracy: 79.6\n",
      "iteration 8470, training loss: 0.31, accuracy: 79.2\n",
      "iteration 8480, training loss: 0.30, accuracy: 72.4\n",
      "iteration 8490, training loss: 0.21, accuracy: 75.2\n",
      "iteration 8500, training loss: 0.21, accuracy: 73.6\n",
      "iteration 8510, training loss: 0.27, accuracy: 75.6\n",
      "iteration 8520, training loss: 0.20, accuracy: 69.2\n",
      "iteration 8530, training loss: 0.26, accuracy: 76.8\n",
      "iteration 8540, training loss: 0.24, accuracy: 79.2\n",
      "iteration 8550, training loss: 0.25, accuracy: 76.0\n",
      "iteration 8560, training loss: 0.20, accuracy: 74.8\n",
      "iteration 8570, training loss: 0.28, accuracy: 75.6\n",
      "iteration 8580, training loss: 0.19, accuracy: 76.4\n",
      "iteration 8590, training loss: 0.23, accuracy: 71.6\n",
      "iteration 8600, training loss: 0.24, accuracy: 74.8\n",
      "iteration 8610, training loss: 0.20, accuracy: 82.0\n",
      "iteration 8620, training loss: 0.21, accuracy: 79.2\n",
      "iteration 8630, training loss: 0.31, accuracy: 72.4\n",
      "iteration 8640, training loss: 0.23, accuracy: 77.2\n",
      "iteration 8650, training loss: 0.25, accuracy: 80.8\n",
      "iteration 8660, training loss: 0.22, accuracy: 76.0\n",
      "iteration 8670, training loss: 0.22, accuracy: 77.2\n",
      "iteration 8680, training loss: 0.22, accuracy: 76.4\n",
      "iteration 8690, training loss: 0.21, accuracy: 76.0\n",
      "iteration 8700, training loss: 0.23, accuracy: 73.2\n",
      "iteration 8710, training loss: 0.23, accuracy: 76.8\n",
      "iteration 8720, training loss: 0.29, accuracy: 74.4\n",
      "iteration 8730, training loss: 0.30, accuracy: 84.8\n",
      "iteration 8740, training loss: 0.23, accuracy: 74.0\n",
      "iteration 8750, training loss: 0.24, accuracy: 80.8\n",
      "iteration 8760, training loss: 0.21, accuracy: 76.4\n",
      "iteration 8770, training loss: 0.30, accuracy: 73.6\n",
      "iteration 8780, training loss: 0.27, accuracy: 75.2\n",
      "iteration 8790, training loss: 0.23, accuracy: 76.0\n",
      "iteration 8800, training loss: 0.29, accuracy: 77.2\n",
      "iteration 8810, training loss: 0.26, accuracy: 72.8\n",
      "iteration 8820, training loss: 0.23, accuracy: 78.4\n",
      "iteration 8830, training loss: 0.23, accuracy: 73.6\n",
      "iteration 8840, training loss: 0.23, accuracy: 77.6\n",
      "iteration 8850, training loss: 0.23, accuracy: 74.0\n",
      "iteration 8860, training loss: 0.23, accuracy: 76.4\n",
      "iteration 8870, training loss: 0.21, accuracy: 77.2\n",
      "iteration 8880, training loss: 0.25, accuracy: 76.0\n",
      "iteration 8890, training loss: 0.19, accuracy: 77.6\n",
      "iteration 8900, training loss: 0.23, accuracy: 74.8\n",
      "iteration 8910, training loss: 0.31, accuracy: 75.6\n",
      "iteration 8920, training loss: 0.22, accuracy: 74.8\n",
      "iteration 8930, training loss: 0.21, accuracy: 77.6\n",
      "iteration 8940, training loss: 0.22, accuracy: 78.4\n",
      "iteration 8950, training loss: 0.28, accuracy: 74.8\n",
      "iteration 8960, training loss: 0.25, accuracy: 73.2\n",
      "iteration 8970, training loss: 0.27, accuracy: 74.4\n",
      "iteration 8980, training loss: 0.41, accuracy: 75.2\n",
      "iteration 8990, training loss: 0.32, accuracy: 78.8\n",
      "iteration 9000, training loss: 0.21, accuracy: 81.6\n",
      "iteration 9010, training loss: 0.21, accuracy: 79.6\n",
      "iteration 9020, training loss: 0.23, accuracy: 79.2\n",
      "iteration 9030, training loss: 0.25, accuracy: 75.6\n",
      "iteration 9040, training loss: 0.22, accuracy: 79.6\n",
      "iteration 9050, training loss: 0.22, accuracy: 70.8\n",
      "iteration 9060, training loss: 0.32, accuracy: 78.0\n",
      "iteration 9070, training loss: 0.21, accuracy: 79.2\n",
      "iteration 9080, training loss: 0.21, accuracy: 80.8\n",
      "iteration 9090, training loss: 0.26, accuracy: 73.6\n",
      "iteration 9100, training loss: 0.22, accuracy: 76.4\n",
      "iteration 9110, training loss: 0.20, accuracy: 74.8\n",
      "iteration 9120, training loss: 0.21, accuracy: 72.0\n",
      "iteration 9130, training loss: 0.28, accuracy: 76.4\n",
      "iteration 9140, training loss: 0.32, accuracy: 77.6\n",
      "iteration 9150, training loss: 0.19, accuracy: 76.4\n",
      "iteration 9160, training loss: 0.25, accuracy: 76.8\n",
      "iteration 9170, training loss: 0.20, accuracy: 78.0\n",
      "iteration 9180, training loss: 0.20, accuracy: 80.8\n",
      "iteration 9190, training loss: 0.20, accuracy: 78.4\n",
      "iteration 9200, training loss: 0.19, accuracy: 78.0\n",
      "iteration 9210, training loss: 0.20, accuracy: 77.6\n",
      "iteration 9220, training loss: 0.20, accuracy: 80.0\n",
      "iteration 9230, training loss: 0.25, accuracy: 81.2\n",
      "iteration 9240, training loss: 0.20, accuracy: 72.8\n",
      "iteration 9250, training loss: 0.20, accuracy: 80.0\n",
      "iteration 9260, training loss: 0.25, accuracy: 80.0\n",
      "iteration 9270, training loss: 0.17, accuracy: 73.6\n",
      "iteration 9280, training loss: 0.26, accuracy: 80.8\n",
      "iteration 9290, training loss: 0.20, accuracy: 75.2\n",
      "iteration 9300, training loss: 0.21, accuracy: 76.0\n",
      "iteration 9310, training loss: 0.21, accuracy: 73.2\n",
      "iteration 9320, training loss: 0.18, accuracy: 73.6\n",
      "iteration 9330, training loss: 0.26, accuracy: 76.0\n",
      "iteration 9340, training loss: 0.22, accuracy: 76.4\n",
      "iteration 9350, training loss: 0.23, accuracy: 79.2\n",
      "iteration 9360, training loss: 0.20, accuracy: 72.8\n",
      "iteration 9370, training loss: 0.18, accuracy: 76.8\n",
      "iteration 9380, training loss: 0.27, accuracy: 75.2\n",
      "iteration 9390, training loss: 0.25, accuracy: 73.2\n",
      "iteration 9400, training loss: 0.26, accuracy: 78.0\n",
      "iteration 9410, training loss: 0.31, accuracy: 79.2\n",
      "iteration 9420, training loss: 0.19, accuracy: 79.2\n",
      "iteration 9430, training loss: 0.36, accuracy: 78.8\n",
      "iteration 9440, training loss: 0.19, accuracy: 77.2\n",
      "iteration 9450, training loss: 0.22, accuracy: 75.2\n",
      "iteration 9460, training loss: 0.22, accuracy: 77.6\n",
      "iteration 9470, training loss: 0.17, accuracy: 77.6\n",
      "iteration 9480, training loss: 0.23, accuracy: 74.4\n",
      "iteration 9490, training loss: 0.20, accuracy: 76.0\n",
      "iteration 9500, training loss: 0.22, accuracy: 78.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9510, training loss: 0.20, accuracy: 79.2\n",
      "iteration 9520, training loss: 0.17, accuracy: 81.6\n",
      "iteration 9530, training loss: 0.23, accuracy: 76.8\n",
      "iteration 9540, training loss: 0.22, accuracy: 74.0\n",
      "iteration 9550, training loss: 0.45, accuracy: 76.8\n",
      "iteration 9560, training loss: 0.34, accuracy: 79.6\n",
      "iteration 9570, training loss: 0.26, accuracy: 75.6\n",
      "iteration 9580, training loss: 0.32, accuracy: 73.2\n",
      "iteration 9590, training loss: 0.25, accuracy: 75.2\n",
      "iteration 9600, training loss: 0.23, accuracy: 81.6\n",
      "iteration 9610, training loss: 0.20, accuracy: 73.2\n",
      "iteration 9620, training loss: 0.20, accuracy: 75.2\n",
      "iteration 9630, training loss: 0.27, accuracy: 77.2\n",
      "iteration 9640, training loss: 0.22, accuracy: 80.0\n",
      "iteration 9650, training loss: 0.19, accuracy: 76.0\n",
      "iteration 9660, training loss: 0.22, accuracy: 75.6\n",
      "iteration 9670, training loss: 0.33, accuracy: 80.0\n",
      "iteration 9680, training loss: 0.24, accuracy: 81.6\n",
      "iteration 9690, training loss: 0.20, accuracy: 75.6\n",
      "iteration 9700, training loss: 0.25, accuracy: 72.8\n",
      "iteration 9710, training loss: 0.32, accuracy: 79.2\n",
      "iteration 9720, training loss: 0.43, accuracy: 80.0\n",
      "iteration 9730, training loss: 0.33, accuracy: 76.4\n",
      "iteration 9740, training loss: 0.22, accuracy: 78.4\n",
      "iteration 9750, training loss: 0.19, accuracy: 81.2\n",
      "iteration 9760, training loss: 0.18, accuracy: 80.0\n",
      "iteration 9770, training loss: 0.21, accuracy: 80.4\n",
      "iteration 9780, training loss: 0.34, accuracy: 74.4\n",
      "iteration 9790, training loss: 0.27, accuracy: 76.4\n",
      "iteration 9800, training loss: 0.25, accuracy: 78.8\n",
      "iteration 9810, training loss: 0.25, accuracy: 75.6\n",
      "iteration 9820, training loss: 0.18, accuracy: 72.4\n",
      "iteration 9830, training loss: 0.25, accuracy: 80.0\n",
      "iteration 9840, training loss: 0.31, accuracy: 74.8\n",
      "iteration 9850, training loss: 0.29, accuracy: 75.6\n",
      "iteration 9860, training loss: 0.28, accuracy: 76.4\n",
      "iteration 9870, training loss: 0.21, accuracy: 78.4\n",
      "iteration 9880, training loss: 0.23, accuracy: 78.4\n",
      "iteration 9890, training loss: 0.26, accuracy: 76.0\n",
      "iteration 9900, training loss: 0.24, accuracy: 82.0\n",
      "iteration 9910, training loss: 0.22, accuracy: 77.6\n",
      "iteration 9920, training loss: 0.22, accuracy: 76.4\n",
      "iteration 9930, training loss: 0.21, accuracy: 80.8\n",
      "iteration 9940, training loss: 0.29, accuracy: 79.2\n",
      "iteration 9950, training loss: 0.26, accuracy: 81.2\n",
      "iteration 9960, training loss: 0.22, accuracy: 82.4\n",
      "iteration 9970, training loss: 0.20, accuracy: 76.0\n",
      "iteration 9980, training loss: 0.20, accuracy: 77.2\n",
      "iteration 9990, training loss: 0.22, accuracy: 79.2\n"
     ]
    }
   ],
   "source": [
    "print(\"Training...\")\n",
    "\n",
    "loss_list, accuracy_list = [], []\n",
    "\n",
    "for i in range(1, n_iter):\n",
    "  \n",
    "    (inputs,targets) = loader.get_batch(batch_size)\n",
    "    loss = siamese_net.train_on_batch(inputs, targets)\n",
    "    \n",
    "    if i % evaluate_every == 0:\n",
    "        val_acc = loader.test_oneshot(siamese_net, N_character, n_val)\n",
    "        \n",
    "        loss_list.append(loss)\n",
    "        accuracy_list.append(val_acc)\n",
    "        \n",
    "        if val_acc >= best:\n",
    "            siamese_net.save(weights_path)\n",
    "            best = val_acc\n",
    "\n",
    "    if i % loss_every == 0:\n",
    "        print(\"iteration {}, training loss: {:.2f}, accuracy: {}\".format(i, loss, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot accuracy\n",
    "\n",
    "plt.plot(accuracy_list)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"% Accuracy\")\n",
    "plt.savefig('accuracy.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accurarcy for 10 x 250 test images = 79.20 %\n"
     ]
    }
   ],
   "source": [
    "print('Average accurarcy for 10 x 250 test images = {0:.2f} %'.format(sum(accuracy_list[-10:])/10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
